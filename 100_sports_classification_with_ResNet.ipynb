{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lIlIlIIIlIIl/Toy_Project/blob/main/100_sports_classification_with_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir results"
      ],
      "metadata": {
        "id": "1p7kk6crh_-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(사전 작업)kaggle.json 파일 다운로드 후 업로드"
      ],
      "metadata": {
        "id": "Tfxw1HLTMC4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "023bSPLVGnzr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d gpiosenka/sports-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd0ZcvPtIv3b",
        "outputId": "32a9a22b-c43a-467a-c68e-48e47e29292b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/gpiosenka/sports-classification\n",
            "License(s): CC0-1.0\n",
            "Downloading sports-classification.zip to /content\n",
            " 98% 417M/424M [00:06<00:00, 87.1MB/s]\n",
            "100% 424M/424M [00:06<00:00, 67.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_file = zipfile.ZipFile('/content/sports-classification.zip') # 압축을 해제할 '/파일경로/파일명.zip'\n",
        "zip_file.extractall('/content/sports-classification') # 압축을 해제할 '/위치경로/'"
      ],
      "metadata": {
        "id": "r6sABsqsLEgJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "rFfuXppgBuyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy # Add Deepcopy for args\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dB4lHsyoBufG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "e-C83h6v-tzO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hj3368fzvmNb"
      },
      "outputs": [],
      "source": [
        "# ===== transform 선언 ===== #\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "# ===== 파일이 다운로드 된 경로 ===== #\n",
        "data_folder = \"/content/sports-classification\"\n",
        "\n",
        "train_folder = data_folder + \"/train\"\n",
        "val_folder = data_folder + \"/valid\"\n",
        "test_folder = data_folder + \"/test\"\n",
        "\n",
        "trainset = datasets.ImageFolder(root=train_folder, transform=transform)\n",
        "valset = datasets.ImageFolder(root=val_folder, transform=transform)\n",
        "testset = datasets.ImageFolder(root=test_folder, transform=transform)\n",
        "\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Code"
      ],
      "metadata": {
        "id": "7BuoZjMTvs2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}"
      ],
      "metadata": {
        "id": "YWZrYZ7CvvZm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model"
      ],
      "metadata": {
        "id": "E8k1koXyvwe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, model_code, in_channels, kernel_size, stride, out_dim, hid_dim, n_linear, act_func, use_bn, dropout):\n",
        "        super(CNN, self).__init__()\n",
        "        self.model_code = model_code\n",
        "        self.in_channels = in_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.out_dim = out_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.act_func = act_func\n",
        "        self.use_bn = use_bn\n",
        "        self.n_linear = n_linear\n",
        "        self.Dropout = nn.Dropout(dropout)\n",
        "        self.padding = self.kernel_size // 2\n",
        "        self.data_size = 224 # 모듈화 필요\n",
        "\n",
        "\n",
        "        if (32 - self.kernel_size + 2*self.padding)/self.stride + 1 != 32:\n",
        "            raise ValueError('filter is not valid')\n",
        "\n",
        "\n",
        "        if self.act_func == 'relu':\n",
        "            self.act_func = nn.ReLU()\n",
        "\n",
        "        elif self.act_func == 'tanh':\n",
        "            self.act_func = nn.Tanh()\n",
        "\n",
        "        elif self.act_func == 'sigmoid':\n",
        "            self.act_func = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError('no valid activation function selected!')\n",
        "\n",
        "        self.layers = self._make_layers()\n",
        "        self.classifier = self._make_fc_layers()\n",
        "\n",
        "    def _make_layers(self):\n",
        "        layers = [] # Sequential을 사용하기 위해 ModuleList가 아닌 그냥 List 사용\n",
        "        for x in cfg[self.model_code]:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(2, 2)]\n",
        "                self.data_size = self.data_size // 2 # 풀링 진행할 때마다 데이터 사이즈 절반으로 축소\n",
        "\n",
        "            else:\n",
        "                layers += [nn.Conv2d(self.in_channels, x, self.kernel_size, self.stride, self.padding)]\n",
        "                if self.use_bn:\n",
        "                    layers += [nn.BatchNorm2d(x)]\n",
        "                layers += [self.act_func]\n",
        "                self.in_channels = x # in_channels 업데이트\n",
        "\n",
        "        self.in_channels = self.in_channels * self.data_size ** 2\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "        # *layers: layers 리스트의 데이터들을 각각 받아주는 문법(가변 인수 느낌)\n",
        "        # Sequential은 리스트를 받아서 하나의 서브모델을 만들어주는 문법. CNN모델 내의 layers모델을 서브모델로 만듦\n",
        "\n",
        "    def _make_fc_layers(self):\n",
        "        linears = []\n",
        "        linears.append(nn.Linear(self.in_channels, self.hid_dim))\n",
        "        for i in range(self.n_linear - 2):\n",
        "            linears += [self.act_func, nn.Linear(self.hid_dim, self.hid_dim),self.Dropout]\n",
        "\n",
        "        linears += [self.act_func, nn.Linear(self.hid_dim, self.out_dim),self.Dropout]\n",
        "        return nn.Sequential(*linears)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x) # layers가 하나의 서브모델\n",
        "        x = x.view(x.size(0), -1) # batch size는 그대로 유지하면서 3차원 tensor를 벡터 형태로 펼쳐준다.\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Pp2oyFx0v0Rc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d06C7AiPCum-"
      },
      "source": [
        "## Dimenstion Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZxGPqj9Cwii",
        "outputId": "f525eab3-9c5c-450b-839a-30a037be127b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 10])\n"
          ]
        }
      ],
      "source": [
        "def dimension_check():\n",
        "    net = CNN('VGG11', 3, 3, 1, 10, 4096, 3,'relu', True, 0)\n",
        "    x = torch.randn(16, 3, 224, 224) # 해당 크기를 갖는 임의의 텐서를 생성\n",
        "    y = net(x)\n",
        "    print(y.size())\n",
        "\n",
        "# layer를 쌓을 때마다 dimension이 맞는지 체크해주는 함수\n",
        "\n",
        "dimension_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "8dk-mON-cwKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "# out_planes는 곧 필터의 개수와 같다.\n",
        "# stride는 1인 경우 사이즈가 유지되고, 2인 경우는 사이즈가 절반으로 줄어든다.\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                     bias=False)"
      ],
      "metadata": {
        "id": "DIjwWT82dTi8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        # inplanes: 인풋 채널 수, planes: 필터 개수\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample # 초기 값과 최종 출력 값의 dimension이 달라진 경우 적용\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x  # 초기 x값을 미리 저장해둠\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x) # 초기 값과 출력 값의 차원이 맞지 않으면, 맞추어주는 작업을 한다.\n",
        "\n",
        "        out += identity # 단순히 출력값과 identity를 더해주면 됨\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "RVI8PPIqc_rj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion) # 필터 개수를 expansion만큼 늘려준다.\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "91qd9xoldGCh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "zheRvQ_HdIh5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kscmVrLlCzrN"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WOoctoVjC3WN"
      },
      "outputs": [],
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
        "                                              batch_size=args.train_batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "    net.train()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다.\n",
        "\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # print(labels)\n",
        "        # print(inputs.size()) # => (batchsize, channel, H, W) = (16, 3, 224, 224). 즉 dataloader가 4차원 텐서 형태로 데이터를 받아놓음; (shape쓰면 안됨)\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS_q0YwXC4hf"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VVT1C04LC5yw"
      },
      "outputs": [],
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
        "                                            batch_size=args.test_batch_size,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1qxk-LBC7yl"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S1X7QJ7bC99F"
      },
      "outputs": [],
      "source": [
        "def test(net, partition, args):\n",
        "    testloader = torch.utils.data.DataLoader(partition['test'],\n",
        "                                             batch_size=args.test_batch_size,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "    return test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FYfQCc2DBjF"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kvo2-xj3DCrP"
      },
      "outputs": [],
      "source": [
        "def experiment(partition, args):\n",
        "\n",
        "    # net = CNN(args.model_code, args.in_channels, args.kernel_size, args.stride, args.out_dim, args.hid_dim, args.n_linear ,args.act_func, args.use_bn, args.dropout)\n",
        "    net = ResNet(args.block, args.layers, args.out_dim, args.zero_init_residual)\n",
        "    net.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "\n",
        "    test_acc = test(net, partition, args)\n",
        "\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "    result['test_acc'] = test_acc\n",
        "    return vars(args), result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):\n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']\n",
        "    del setting['test_batch_size']\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
        "    result.update(setting)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)\n",
        "\n",
        "\n",
        "def load_exp_result(exp_name):\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        if exp_name in filename:\n",
        "            with open(join(dir_path, filename), 'r') as infile:\n",
        "                results = json.load(infile)\n",
        "                list_result.append(results)\n",
        "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "    return df"
      ],
      "metadata": {
        "id": "YP03HcxI_tqe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp8\"\n",
        "\n",
        "# ====== Model Capacity ====== #\n",
        "args.out_dim = 100\n",
        "args.hid_dim = 512\n",
        "args.n_linear = 2\n",
        "args.act_func = 'relu'\n",
        "args.model_code = 'VGG13'\n",
        "args.in_channels = 3\n",
        "args.kernel_size = 3\n",
        "args.stride = 1\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.l2 = 0.0001\n",
        "args.use_bn = True\n",
        "args.dropout = 0.3\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.001\n",
        "args.epoch = 15\n",
        "\n",
        "args.train_batch_size = 32\n",
        "args.test_batch_size = 32\n",
        "\n",
        "# ====== Experiment Variable ====== #\n",
        "name_var1 = 'act_func'\n",
        "name_var2 = 'optim'\n",
        "list_var1 = ['relu', 'tanh', 'sigmoid']\n",
        "list_var2 = ['SGD', 'RMSprop', 'Adam']\n",
        "\n",
        "# ===== ResNet Parameter ===== #\n",
        "args.block = Bottleneck\n",
        "args.layers = [3, 4, 23, 3]\n",
        "args.zero_init_residual = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for var1 in list_var1:\n",
        "    for var2 in list_var2:\n",
        "        setattr(args, name_var1, var1)\n",
        "        setattr(args, name_var2, var2)\n",
        "        print(args)\n",
        "\n",
        "        setting, result = experiment(partition, deepcopy(args))\n",
        "        save_exp_result(setting, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLXr3vr0wyq0",
        "outputId": "54eecfe8-239c-4fa4-9f51-852df3d80a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(exp_name='exp8', out_dim=100, hid_dim=512, n_linear=2, act_func='relu', model_code='VGG13', in_channels=3, kernel_size=3, stride=1, l2=0.0001, use_bn=True, dropout=0.3, optim='SGD', lr=0.001, epoch=15, train_batch_size=32, test_batch_size=32, block=<class '__main__.Bottleneck'>, layers=[3, 4, 23, 3], zero_init_residual=False)\n",
            "Epoch 0, Acc(train/val): 1.47/2.00, Loss(train/val) 4.60/4.58. Took 232.97 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a_coxkDyM5x",
        "outputId": "2ec66fac-9150-4cbc-b1e7-632f565d44fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 7            |        cudaMalloc retries: 7         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14955 MiB |  14955 MiB |  14956 MiB |    883 KiB |\n",
            "|       from large pool |  14942 MiB |  14942 MiB |  14942 MiB |      0 KiB |\n",
            "|       from small pool |     13 MiB |     13 MiB |     14 MiB |    883 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14955 MiB |  14955 MiB |  14956 MiB |    883 KiB |\n",
            "|       from large pool |  14942 MiB |  14942 MiB |  14942 MiB |      0 KiB |\n",
            "|       from small pool |     13 MiB |     13 MiB |     14 MiB |    883 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14949 MiB |  14949 MiB |  14950 MiB |    882 KiB |\n",
            "|       from large pool |  14936 MiB |  14936 MiB |  14936 MiB |      0 KiB |\n",
            "|       from small pool |     13 MiB |     13 MiB |     14 MiB |    882 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14968 MiB |  14968 MiB |  14968 MiB |      0 B   |\n",
            "|       from large pool |  14954 MiB |  14954 MiB |  14954 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  12825 KiB |  32016 KiB | 174220 KiB | 161395 KiB |\n",
            "|       from large pool |  12288 KiB |  30368 KiB | 165152 KiB | 152864 KiB |\n",
            "|       from small pool |    537 KiB |   2041 KiB |   9068 KiB |   8531 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     495    |     495    |     498    |       3    |\n",
            "|       from large pool |      46    |      46    |      46    |       0    |\n",
            "|       from small pool |     449    |     449    |     452    |       3    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     495    |     495    |     498    |       3    |\n",
            "|       from large pool |      46    |      46    |      46    |       0    |\n",
            "|       from small pool |     449    |     449    |     452    |       3    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      26    |      26    |      26    |       0    |\n",
            "|       from large pool |      19    |      19    |      19    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       9    |      11    |      20    |      11    |\n",
            "|       from large pool |       6    |       7    |      13    |       7    |\n",
            "|       from small pool |       3    |       5    |       7    |       4    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_acc(var1, var2, df):\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3)\n",
        "    fig.set_size_inches(15, 6)\n",
        "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
        "\n",
        "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
        "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
        "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
        "\n",
        "    ax[0].set_title('Train Accuracy')\n",
        "    ax[1].set_title('Validation Accuracy')\n",
        "    ax[2].set_title('Test Accuracy')\n",
        "\n",
        "\n",
        "def plot_loss_variation(var1, var2, df, **kwargs):\n",
        "\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_losses = list(row.train_losses)[0]\n",
        "            val_losses = list(row.val_losses)[0]\n",
        "\n",
        "            for epoch, train_loss in enumerate(train_losses):\n",
        "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_loss in enumerate(val_losses):\n",
        "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train loss vs Val loss')\n",
        "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
        "\n",
        "\n",
        "def plot_acc_variation(var1, var2, df, **kwargs):\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_accs = list(row.train_accs)[0]\n",
        "            val_accs = list(row.val_accs)[0]\n",
        "            test_acc = list(row.test_acc)[0]\n",
        "\n",
        "            for epoch, train_acc in enumerate(train_accs):\n",
        "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_acc in enumerate(val_accs):\n",
        "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
        "\n",
        "    def show_acc(x, y, metric, **kwargs):\n",
        "        plt.scatter(x, y, alpha=0.3, s=1)\n",
        "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
        "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
        "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
        "\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
        "    plt.subplots_adjust(top=0.89)"
      ],
      "metadata": {
        "id": "qYHZAUX6K5Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var1 = 'act_func'\n",
        "var2 = 'optim'\n",
        "df = load_exp_result('exp8')\n",
        "\n",
        "plot_acc(var1, var2, df)\n",
        "plot_loss_variation(var1, var2, df, sharey=False) #sharey를 True로 하면 모둔 subplot의 y축의 스케일이 같아집니다.\n",
        "plot_acc_variation(var1, var2, df, margin_titles=True, sharey=True) #margin_titles를 True로 하면 그래프의 가장자리에 var1과 var2 값이 표시되고 False로 하면 각 subplot 위에 표시됩니다."
      ],
      "metadata": {
        "id": "clHNWLkmLAIq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}